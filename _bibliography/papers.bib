@InProceedings{varricchione2023synthesising,
author="Varricchione, Giovanni
and Alechina, Natasha
and Dastani, Mehdi
and Logan, Brian",
editor="Malvone, Vadim
and Murano, Aniello",
title="Synthesising Reward Machines for Cooperative Multi-Agent Reinforcement Learning",
booktitle="Proceedings of the 20th European Conference on Multi-Agent Systems (EUMAS 2023)",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="328--344",
abstract="Reward machines have recently been proposed as a means of encoding team tasks in cooperative multi-agent reinforcement learning. The resulting multi-agent reward machine is then decomposed into individual reward machines, one for each member of the team, allowing agents to learn in a decentralised manner while still achieving the team task. However, current work assumes the multi-agent reward machine to be given. In this paper, we show how reward machines for team tasks can be synthesised automatically from an Alternating-Time Temporal Logic specification of the desired team behaviour and a high-level abstraction of the agents' environment. We present results suggesting that our automated approach has comparable, if not better, sample efficiency than reward machines generated by hand for multi-agent tasks.",
isbn="978-3-031-43264-4",
abbr={EUMAS},

pdf={eumas23-marm.pdf},

}

@InProceedings{varricchione2024purepast,
author="Varricchione, Giovanni
and Alechina, Natasha
and Dastani, Mehdi
and De Giacomo, Giuseppe
and Logan, Brian
and Perelli, Giuseppe",
title="Pure-Past Action Masking",
year=2024,
booktitle="Proceedings of the 38th AAAI Conference on Artificial Intelligence (AAAI 2024)",
notes="Safe, Robust and Responsible AI track (\emph{to appear})",
abbr={AAAI},
abstract="We present Pure-Past Action Masking (PPAM), a lightweight approach to action masking for safe reinforcement learning. In PPAM, actions are disallowed (``masked'') according to specifications expressed in Pure-Past Linear Temporal Logic (PPLTL). PPAM can enforce non-Markovian constraints, i.e., constraints based on the history of the system, rather than just the current state of the (possibly hidden) MDP. The features used in the safety constraint need not be the same as those used by the learning agent, allowing a clear separation of concerns between the safety constraints and reward specifications of the (learning) agent. We prove formally that an agent trained with PPAM can learn any optimal policy that satisfies the safety constraints, and that they are as expressive as shields, another approach to enforce non-Markovian constraints in RL. Finally, we provide empirical results showing how PPAM can guarantee constraint satisfaction in practice."

pdf={aaai24-ppam.pdf},
poster={aaai24-ppam-poster.pdf},
}